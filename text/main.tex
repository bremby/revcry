
\documentclass[a4paper,10pt,openright]{memoir}

% Style of front page, title page, and stuff page
\usepackage{dikuReport}


% Packages
\usepackage[utf8]{inputenc}
\usepackage{latexsym}
\usepackage[T1]{fontenc}
\usepackage{cite}
\usepackage{pdfpages}
\usepackage{tikz}
\usepackage{fix-cm}
\usepackage{xcolor,calc}
\usepackage{graphicx}
\usepackage{amsbsy}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{alltt}
\usepackage{float}
\usepackage{color}
\usepackage{relsize}
\usepackage{calc}
\usepackage{ifthen}
\usepackage{xspace}
\usepackage{listings}
\usepackage{todonotes}
% Janus listings
\usepackage{lstjanus}



\lstset{language=C,
        basicstyle=\footnotesize\ttfamily,
        keywordstyle=\bfseries\color{blue},
        stringstyle=\color{green}\ttfamily,
        commentstyle=\color{red}\ttfamily,
        %morecomment=[l][\color{magenta}]{\#}
        columns=flexible,
        keepspaces=true
}

\lstset{language=Python,
        basicstyle=\footnotesize\ttfamily,
        keywordstyle=\bfseries\color{blue},
        stringstyle=\color{green}\ttfamily,
        commentstyle=\color{red}\ttfamily,
        %morecomment=[l][\color{magenta}]{\#}
        columns=flexible,
        keepspaces=true
}

%%%%% Make abbreviations emphasized.
\newcommand{\ie}{\emph{i.e.}\xspace}
\newcommand{\eg}{\emph{e.g.}\xspace}
\newcommand{\etc}{\emph{etc.}\xspace}
\newcommand{\vs}{\emph{vs.}\xspace}
\newcommand{\cf}{\emph{cf.}\xspace}
\newcommand{\viz}{\emph{viz.}\xspace}
\newcommand{\etal}{\emph{et~al.}\xspace}

\def\enc{\ensuremath{\mathit{enc}}}
\def\dec{\ensuremath{\mathit{dec}}}
\newcommand{\inv}[1]{\ensuremath{\mathcal{I}(#1)}}
\newcommand{\exe}[1]{\ensuremath{[\![#1]\!]}}

\newcommand{\name}[1]{\textsc{#1}}
\newcommand{\term}[1]{\textit{#1}}
\newcommand{\code}[1]{\texttt{#1}}

\makeatletter
\newcommand{\verbatimfont}[1]{\def\verbatim@font{#1}}%
\makeatother


% Include some layout setup.
\input{layout.tex}

% Sets page numbering layout to normal
\normalPN

% Where graphic files are located
\graphicspath{{figures/}}

% Alters the margins of the pages such that text are _not_ centered. This makes better room for the glue bindings.
\addtolength{\foremargin}{-45pt}
\addtolength{\spinemargin}{45pt}
\checkandfixthelayout



\begin{document}

\verbatimfont{\ttfamily\small}%

% Basic information
\thesistype{MSc thesis}
\thesiscomment{} % You can leave this blank
\title{Lightweight Crypto in Reverse}
\subtitle{Exploring lightweight cryptography in the context of reversible computing}
\author{Dominik T\'{a}borsk\'{y}}
\supervisor{Michael Kirkedal Thomsen, Ken Friis Larsen}
%\date{July 7, 2018} % Hand-in date <<<------------------------<<<-----------------<<<-----------------!!!!!! FILL IN !!!!<<<-------------
%\subject{The short description that is suitable for a database.} % This is not needed.

% Make the front page, title page, and other required information.
\pagestyle{plain}
\maketitle

% Start at page 3. I do not count the front page in the numbering.
\cleardoublepage
\pagenumbering{roman}
\setcounter{page}{3}

% English Abstract
\cleardoublepage
\pagestyle{plain}
\begin{abstract}

The aim of this thesis is to look at cryptography in a reversible 
computing environment. Reversibility works on the principle that no 
information can be destroyed during the computation, so any computation 
can be traced back to the beginning. In a clean setting no information 
is also unintentionally copied. These features have the implication 
that any residual information is either known (e.g. null-initialized 
memory stays zero at the end of the computation, or constants keep 
their value), or the encryption implementation can be potentially 
attacked by using the residual information to lower the searched 
entropy for brute-force decryption.

In an emerging classification of encryption algorithms, we 
consider \textit{lightweight encryption} algorithms for several 
reasons: reversible computation has a direct relation to low-power 
applications, and also the current breed of programming languages is 
still rather experimental; standard practices have not evolved yet, so 
preferably simpler algorithms shall be considered first.

\end{abstract}

% Danish abstract
% \clearpage
% \begin{resume}
% Dansk resum\'e
% \end{resume}

% Table of contents
\cleardoublepage
\chapterstyle{combined}
\tableofcontents*


% Starting the real text.
\cleardoublepage
\pagenumbering{arabic}
\setcounter{page}{1}


%\cftaddnumtitleline{toc}{chapter}{Preface}{}{}%
\cftaddnumtitleline{toc}{chapter}{Preface}{}{\arabic{page}}%
\chapter*{Preface}

The present thesis constitutes a 30 ECTS workload and is submitted in 
partial fulfilment of the requirements for the degree of Master of 
Science in Computer Science at the University of Copenhagen (UCPH), 
Department of Computer Science (DIKU).\\

In addition to this text and its appendices, a GitHub repository is 
available\cite{revcry-github} containing all the related files that are 
being discussed here.\\

I would like to thank my girlfriend, my new friends in Copenhagen and 
mainly my supervisors for their enormous support. I have to emphasize 
the influence Ken and Michael had over me and my work and their 
friendly and understanding approach to obstacles on the way. I thank 
them immensely.\\
\\
Copenhagen and \v{C}esk\'{e} Bud\v{e}jovice, Summer 2018\\
\\
Dominik T\'{a}borsk\'{y}


\cleardoublepage
%%\cftaddnumtitleline{toc}{chapter}{Preface}{}{}%
%\cftaddnumtitleline{toc}{chapter}{List of Abbreviations}{}{\arabic{page}}%
%\chapter*{List of Abbreviations}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% INTRODUCTION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\cleardoublepage

\chapter{Introduction}

%Overview of the context, current problems and difficulties, discussion 
%of current solutions, our solution and its short evaluation.

Cryptography has been with us for decades now, yet only 
recently we figured out how to reliably code and verify an 
actual implementation with the \term{F*/HACL} work \cite{ProtzenkoEtal:2017}. 
A similar approach takes the \term{Vale} project \cite{BondEtal:2017:Vale}, 
designing a new language that utilizes either one of \term{Dafny} or \term{F*} languages for 
verification. These solutions verify not only the correctness of the 
code, but also prove the lack of state-based information leakage, and 
in the case of Vale, even timing-based leakage. These two are possibly 
the easiest side-channel vulnerabilities to exploit, especially given 
the fact that physical access to the computer is not necessary. Both of 
them can get difficult to control for since the compiler often provides 
no guarantees on the generated machine code. Similarly, general high-level programming 
languages do not provide semantics on low enough level to account for 
registers, caches and execution timing. Vale provides for these, which 
allows it to formally verify the required cryptographic properties.

The range of cryptographic functions is significantly broad, so we are 
limiting the scope to only lightweight cryptography. The core concepts 
are the same, \ie side-channel vulnerabilities still apply in the same 
way, but the algorithms can be a lot simpler. Secondly, lightweight 
cryptography is interesting in its own sense due to its own application 
within \term{Internet of Things (IoT)} systems. These systems are 
limited in processor performance, power consumption, memory size, 
connectivity and even reaction times (real-time systems). In such 
cases, lightweight cryptography is useful in providing enough security 
even with limited resources.

Our work aims for exploiting reversible computing to reason about 
state-based leakage. Informally, a reversible program has to follow a 
clearly defined path that transforms the input data to output data 
without losing information. Because of that lack of information loss, 
it can be described as an injective function. Encryption algorithms are 
bijective functions, implying they can be directly encoded in a 
reversible setting. In a formal description, we have

$$
\exe{\enc}(k,p) = (k,c) \,.
$$

The plaintext $p$ is directly transformed to a ciphertext $c$, while 
key $k$ remains the same. Note that the key cannot be omitted: either 
such the function would not be injective, thus irreversible, or the 
ciphertext would somehow contain the key, which would make the 
encryption useless by making the key public. 

In Janus we enforce this scheme by offloading any other data (\eg algorithm-specific magic constants or iterators) into 
temporary local variables. These variables are declared and defined at 
the top of a function and undeclared at the bottom, with their final 
value specified. This final value is necessary for reversibility: 
without it, such reversed function would have undefined initial values. 
Secondly, because we know the final value of each variable, that value 
can be subtracted from the variable, thus clearing it. This implies 
that local data, not part of the input and output, will be cleared and 
thus no state-based leakage is possible. We demonstrate this by 
translating a Janus program to C and executing it within a virtual 
environment that tracks secret data (see chapter \ref{sec:eval}).

Furthermore, thanks to reversibility we implement only the encryption 
functions. The decryption is then obtained by reversing the encryption. 
This decreases time spent on development, debugging, testing and 
showing the duality of encryption and decryption functions.

Our conclusion is that reversibility is a handy tool for developing not 
only cryptographic code, but also other injective functions. It gives 
no guarantees on avoiding timing vulnerabilities like Vale does, 
however it comes at a much lower cost of development. We also take note 
of the difficulty to learn and use Vale due to the sheer breadth of 
details the programmer has to consider, instead of leaving it to the 
compiler.





%The reason for 
%vulnerabilities based on the implementation and not the mathematical 
%basis of the algorithm is usually performance, simplicity, inexperience 
%and even language and/or compiler support for low-level semantics.




\section{Goals}
\label{sec:goals}

Our aim is to explore the combination of reversibility and 
cryptography. Irreversible implementations of crypto algorithms often 
either suffer from insufficiencies from the security perspective or are 
difficult to implement correctly. Due to performance being also 
critical, programmers have to actively fight compilers to make sure 
their code is safe. Our main goal is to alleviate that in at least some 
way. Reversible code is limited in how it manages memory - no erasure 
is allowed, and so deallocations have to be preceded by proper 
cleanup. Reversible algorithms can follow a much cleaner formal 
definition that does not leak memory contents outside the scope of 
those functions.

The second goal is to exploit the bijection of crypto algorithms. 
Decryption is just an inverse function to encryption, so only one of them 
needs coding. This lowers the implementation and debugging time demands.

The third goal is getting some practical experience with Janus, finding 
its strengths and weaknesses and suggesting improvements.

\section{Problem statement}

Classic irreversible implementations of encryption schemes suffer from 
human errors making them vulnerable to side-channel exploits. 
Reversible implementation removes one type of side-channel and has 
potential to alleviate the cost of development and debugging.

\section{Overview of the chapters}

Apart from the Introduction, these are the chapters and their short 
summaries this thesis builds upon:

\begin{description}

\item[Analysis] Presents short introduction to cryptography, discusses 
the practical problems with side-channel vulnerabilities, and explores 
reversible computing and its implications in the context of 
cryptography.

\item[Design] Talks about how we approach the problem with a selection 
of algorithms, their expected behaviour and how we can verify a lack of 
state leakage.

\item[Implementation] This chapter focuses on writing the selected 
algorithms in Janus and the practical experience of it.

\item[Evaluation] Here we discuss how we verify our claims, then 
suggest and perform some experiments to do that. We also look at how we 
can work around the problem of not having a fully reversible platform.

\item[Reflection and future work] We reflect on how well our solution 
can work in practice, and what else needs to or should be done. \todo{do so}

\end{description}

\chapter{Analysis}

%In this chapter we look at lightweightedness in cryptography, consider 
%fundamental properties of reversibility, analyze properties of our 
%solution{ and define terminology}.

\section{Symmetric and asymmetric cryptography}

Cryptographic algorithms can be defined as functions that take two 
parameters, a key and some data, perform either encryption or 
decryption and return the key and the modified data. Normally the key 
in the result would be omitted, however it is useful to recognize the 
fact that the key is not altered in any way. This will become useful 
later on.

Symmetric cryptography differs from the asymmetric kind by its use of 
only a single key for both encryption and decryption. These are the 
classic algorithms like AES, RC5 and Chacha20. They also tend to be 
much more efficient performance-wise than their asymmetric siblings. 
This efficiency also lead to establishing the subarea of 
\term{lightweight cryptography}. Algorithms classified as 
\term{lightweight} are generally good candidates for use in 
\term{Internet of Things (IoT)} devices, which often have low 
computational performance, need to limit power usage, have strict 
memory requirements, or even may be required to react quickly. A 
special kind of symmetric cryptographic algorithms are stream ciphers, 
which only generate seemingly random output which is then combined with 
the input data (plaintext or ciphertext) using some self-inverting 
operation, typically XOR. Therefore stream ciphers only implement this 
"random" output and not individual encryption and decryption functions, 
since those are trivial and equivalent.

Asymmetric cryptography, also known as \term{Public Key Cryptography 
(PKI)}, uses one key for encryption (a \term{public key}) and another 
for decryption (a \term{private key}). Its security relies on some hard 
mathematical problems like integer factorization in the case of RSA. If 
that could somehow be performed as fast as the factor multiplication, 
it would render the whole algorithm broken. It does not achieve the 
same levels of performance \cite{sym-asym} and for that reason it is not 
considered lightweight. It can, however, be used for so-called hybrid 
encryption schemes, where PKI is used to exchange the secret key for 
some symmetric algorithm.

\subsection{Definition}

Formally we use the following notation to describe a function 
$\enc_S$ and its inverse $\dec_S$ that fits the description above:

\begin{align*}
\enc_S(k,p) = (k,c) \\
\dec_S(k,c) = (k,p) 
\end{align*}

This suffices to describe symmetric cryptographic algorithms including 
stream ciphers where $\enc = \dec$.

For asymmetric algorithms we have to differentiate between public and 
private keys:

\begin{align*}
\enc_A(k_\text{public},p) = (k_\text{public},c) \\
\dec_A(k_\text{private},c) = (k_\text{private},p) 
\end{align*}

However, notice one major difference between the encryption functions 
$\enc_S$ and $\enc_A$: the resulting pair $(k, c)$ of $\enc_S$ contains 
all necessary information to decrypt the ciphertext back using 
$\dec_S$. That is not the case with $\enc_A$: having access to 
$(k_\text{public},c)$ does not suffice for decryption, since we are 
missing the private key $k_\text{private}$. This is an important 
distinction that we will explore later in \ref{sec:asym_rev}. In short, 
the implication is that the definition of $\enc_A$ is not as complete 
as $\enc_S$.

\section{Vulnerabilities in cryptographic code}

The mathematical basis upon which cryptographic algorithms are built is 
only one half of the story when it comes to evaluating data security. 
The other half are their implementations, which have a long history of 
hidden vulnerabilities in various forms. These are often split into two 
groups\footnote{A word on the classification: the grouping provided 
here follows other sources. From a second perspective, state-based 
leakages can also be considered a side-channel, since the mathematical 
concept of cryptography does not deal with how memory is handled. We 
can counter such argument by saying that the state, especially the key 
and the plaintext, should remain secret no matter the code. 
Nevertheless, the classification is only secondary for our 
purposes.}:incorrect implementation (software bugs, state-based 
leakages) and so-called \term{side-channel vulnerabilities}, 
\term{side-channel information leakages} or just \term{side-channels} 
for short. The first group consists of attacks on weaknesses in the 
software implementation, mostly state information leakage via registers 
and/or memory, \ie not clearing memory containing sensitive data 
\cite{Chow:2004}. The second group is classified into several subgroups 
depending on the type of the attack, \eg power consumption and 
electromagnetic radiation analyses \cite{Spreitzer:2016}. A 
timing-based attack exploits different running times of the algorithm 
depending on its inputs. Arguably, this kind of attack could be 
classified as both a software bug, since it can be often circumvented 
in code, but the source of the problem often lies within the processor 
itself (instructions themselves being timing-sensitive based on input 
\cite{TimingRC5}).

The easiest (in terms of required prerequisites) are state-based and 
timing-based leakages, since they can be performed remotely and do not 
require any specialized hardware. Because of that, they tend to be the 
focus of contemporary research in the area 
\cite{BondEtal:2017:Vale}\cite{whatyouc}.

\subsection{Examples}

The common issue with state-based information leakage is that either 
the programmer does not clear the memory that contains the sensitive 
data (\eg a password) and simply deallocates it, or that they do clear 
it in the code, but the compiler removes that code because it is 
considered a dead (ineffective) code.

\begin{lstlisting}
int login()
{
  int ret = 0;
  char *password = ask_for_password();
  //ret = ... use the password
  memset(password, 0, strlen(password));
  return ret;
}
\end{lstlisting}

In the above example, the memory pointed to by the \code{password} 
variable is supposed to be cleared out using the \code{memset} 
function, but since the memory is not being used anymore, optimizing 
compilers remove that call completely\cite{whatyouc}.

An example of a timing vulnerability is in the case of the RC5 
algorithm \cite{TimingRC5}, which uses bitwise rotation, where the 
number of bits rotated depends on the input data. The vulnerability 
exists only on some platforms, which differ from the others in the 
implementation of the rotation instruction. The rotation can be 
performed in two ways: 
\begin{enumerate}
\item perform one rotation by $n$ bits,
\item perform $n$ rotations by 1 bit.
\end{enumerate}
This minor change can have drastic effects on security.

\subsection{Avoidance}

There are multiple ways for avoiding side-channel vulnerabilities, but 
most (if not all) of them require some extra work on the programmer's 
side. The most time consuming and also error-prone method for avoiding 
state leakage would be manually clearing memory. Just tracking the used 
memory would be exhausting. Naturally, the programmer could create a 
\term{memory pool}, \ie a block of memory, where the algorithm would 
store its data. This block would then be erased altogether after the 
algorithm has finished. Along with that, the programmer would also need 
to erase the stack and processor registers. A reliable way of doing 
that is extending a compiler, which is described in \cite{whatyouc} and 
which we also use (see section \ref{sec:verifmet} and later 
\ref{sec:eval}).

Avoiding a timing side-channel is significantly harder, since it 
depends on the processor instruction semantics and may depend on the 
algorithm. There has been some progress with this, and we discuss this 
in \ref{sec:verifmet}. Every side-channel is specific, however, and 
requires specific countermeasures. In case of the power analysis, for 
example, the actual device has to be designed from the ground up to be 
resilient.



\section{Reversibility and its implications}

Reversibility is the ability of a program to be executed both forwards 
and backwards deterministically. Although discussed earlier, first 
major research was done by Landauer\cite{Landauer61}. 
Bennett\cite{Bennett73} and others continued the research and a first 
logically reversible language Janus was born (\cite{janus86}, 
\cite{Yokoyama2010}).

\subsection{Definitions}

Consider a reversible program $p$ and some values $x$ and $y$. Then 
$\exe{p}(x) = (y)$ represents\footnote{Here we are using the notation 
from \cite{Jones93}.} the execution of program $p$ with $x$ as a given 
parameter and $y$ being the result. So in the case of encryption we 
have:

$$
\exe{\enc}(k,p) = (k,c) \,.
$$

Note that before $\enc$ represented a function in a mathematical sense, 
but in this case it is a program composed of some instructions. The 
$\exe{\cdot}$ operator stands for the forward interpretation of the 
given program. For backward interpretation we use the inversion 
operator $\inv{\cdot}$, which takes a program and inverts it, so that 
$\exe{\inv{\cdot}}$ runs the given program backwards. Now we clearly 
get that:

$$
\exe{\inv{p}}(\exe{p}(x)) = x \,.
$$


\subsection{Cryptography as a reversible embedding}
\label{crypto-embed}

Some cryptographic functions are partial functions, but we can always 
limit their domains to valid inputs. We will also assume these 
functions are injective. Altogether, we can assume to work with 
bijective functions. With this assumption, we can already make one 
interesting observation:

\begin{align*}
\exe{\enc}(k,p) &= (k,c) \\
\exe{\dec}(k,c) &= \exe{\inv{\enc}}(k,c) = (k,p) \,.
\end{align*}

The implication of that is that we can avoid implementing the 
decryption functions after having implemented the encryption ones (and 
vice-versa). In practical experience, this fact is easily observable 
and immensely useful. It has several advantages:

\begin{enumerate}
\item time saved on implementing the inverse function,
\item time saved on debugging it and making sure it actually is the correct inverse
\item easier maintenance and changes
\item smaller source files (and potentially even binary files)
\end{enumerate}

We can see examples in the code developed alongside this thesis; no 
algorithm required separate encryption/decryption functions.

\subsection{Expressive power}
\label{sec:expr-power}

Bennett has formalized \cite{Bennett73} a reversible Turing machine 
using deterministic and reversible operations. Quite clearly the 
expressive power of the reversible Turing machine is not larger than of 
the irreversible one; irreversibility allows for information erasure, 
it does not require it. Similarly, the power is not smaller. This can 
be shown by having the irreversible machine keep a log of its 
operations and intermediate values, run its algorithm to finish to 
compute the output, copy the output (without logging) and then run all 
its operations backwards using the log. The complete proof was also 
shown by Bennett \cite{Bennett73}. Below is a simple example for 
addition:

\begin{lstlisting}[language=c]
  state: x=3, y=5, z=0, output=0  // initial state
  log: empty
  z = (x + y)                     // original algorithm
  state: x=3, y=5, z=8, output=0  // forward run finished
  log: set z to (x + y), was 0
  output = z                      // copy result to a safe location, log ignored
  state: x=3, y=5, z=8, output=8  // copy saved
  log: set z to (x + y), was 0
  z = 0                           // run original algorithm backwards
  state: x=3, y=5, z=0, output=8  // final state with initial input values, 
                                  // correct result and no intermediate values
  log: empty
\end{lstlisting}




\subsection{Reversing asymmetric cryptography}
\label{sec:asym_rev}

When we formally defined functions $\enc_S$ and $\enc_A$, we noticed 
the distinction in what information they give as a result. After 
looking at reversibility and seeing what it provides for symmetric 
cryptography, naturally it may seem like reversibility breaks 
asymmetric cryptography since an attacker knows both values, the public 
key and the ciphertext. We also know that reversibility does not 
increase expressive power of a language, it is just as powerful as a 
general Turing machine. So where is the problem?

The problem is that our definition was not complete. If the pair of the 
public key and the ciphertext contained all the information, then the 
algorithm would not provide any security. The security is exactly in 
the fact that the attacker has to search for some hidden information 
that is not available to them. One way of looking at this is simply 
considering multiplication: multiplying 3 and 4 has only one result, 
but if we know the result is 12, we do not know what the factors were. 
If we knew one of them was 3, then we would have all the information 
and could just calculate the other one. With irreversible setting this 
is not a problem, we delete information all the time, but that is not 
an option with reversible languages.

The correct and complete definition of $\enc_A$ would then be:

\begin{align*}
\enc_A(k_\text{public},p) = (k_\text{public}, p, c) \\
\end{align*}

Of course the plaintext is not transmitted or shared, only the 
ciphertext is. This definition avoids information loss and can be 
directly reversed by simply uncalculating\footnote{By "uncalculating" 
we mean reversing the program execution, thus going from the output 
back to the input. See also Bennett's tricks later in this chapter.} 
the ciphertext. In the case of the RSA algorithm, the public key has 
two components which are combined with the plaintext input in a 
loss-ful way. How this operation works is well-understood, out-of-scope 
of this thesis and the details are not important for us here.

\subsection{Data transformation}

Going back to section \ref{crypto-embed}, the second mentioned 
advantage of the duality of encryption and decryption functions, and 
similarly with other bijective functions that can be reversed like 
this, should not be underestimated. The world of programming languages 
is constantly trying to improve to make the job easier for programmers. 
Just like some languages are more useful (or at least more common) in 
certain fields (\eg C for low-level systems, Haskell for parsing, pure 
computer science and language development, C++ for performance-hungry 
video games and Java for enterprise applications), reversible languages 
could create a new such field. Algorithms that perform some kind of 
data transformation - mathematically bijective or at least injective 
function application - would be a good fit. The reversibility would 
then assist with error recovery, for example; it could prove useful in 
transactional systems, which require the ability to roll back aborted 
transactions.

\subsection{Bennett's tricks}

In section \ref{sec:expr-power} we have already seen the first trick 
for converting irreversible programs into reversible ones. The trick is 
fairly simple: to get a result of any function that may create and 
destroy any number of bits of information, simply provide temporary 
storage for those bits, run the function without erasing anything until 
completion, copy out the result and finally run the function backwards, 
uncomputing the temporary values in the process. This trick was used 
several times in the cryptographic code developed for this thesis. The 
\term{Speck} algorithm is a good example of that, as it requires to 
perform a fairly complicated key expansion process. There are a few 
other places where this trick was used, but always only in cases where 
making a more direct reversible code - that would be equivalent to the 
originally intended meaning - was either impossible or too difficult.

There also exists a second trick that is not immediately obvious. Having 
a reversible program $p$ and some input $x$ and corresponding output $y$, the 
following holds:

\begin{align*}
\exe{p}(x) = y
\exe{\inv{p}}(y) = x
\end{align*}

This is just the definition of reversibility. However, the trick is in 
how this is used: suppose the reversible program $p$ computes its input 
into output without loss of information. We can store $x$ and $y$ at 
different memory locations, so that the information within $x$ is 
duplicated in $y$. If $x$ is not needed anymore, we can uncompute it by 
executing the inverse of $p$ with $y$ as its input and $x$ as its 
output. This method is also described in \cite{Bennett73}.

\begin{align*}
&\text{state: } a = 3, b = 0, p(x,y) = \lambda x. \lambda y. y += (x + 5)\\
&\exe{p}(a,b)\\
&\text{state: } a = 3, b = 8, \inv{p}(x,y) = \lambda x. \lambda y. y -= (x - 5)\\
&\exe{\inv{p}}(y,x)\\
&\text{state: } a = 0, b = 8\\
\end{align*}

%\begin{lstlisting}[language=c]
  %state: x = 3, y = 0, f(x) = x + 5 // initial state
  %y += (x + 5) f(x)    // original algorithm
  %state: x = 3, y = 8, // forward run finished
  %state: x = 3, y = 5, // copy saved
  %x -= f^-1(y)         // run original algorithm backwards
  %state: x = 3, y = 5, output = 0, output_copy = 8  // final state with initial input values, the correct result and no intermediate values (empty log)
%\end{lstlisting}

This second trick was used only in the Salsa20 algorithm in the 
\code{quarterround} procedure. 

\subsection{Reversible languages}

There have been several reversible languages designed (a short summary 
is provided at the end of \cite{Yokoyama2010}), but so far \term{Janus} 
has proven most useful. It has an actively developed interpreter in 
Haskell, another interpreter in JavaScript, which enables running code 
in a web browser and it was also extended several times 
\cite{janus-topps}. Janus is a (semi-)high-level C-like language, which 
makes it a fit for comparing implementations with actual C code. Author 
also has to acknowledge the fact that one of the supervisors is the 
developer of the interpreter in Haskell, and our cooperation has 
brought many patches, updates and extensions to it.

Another language that has been considered for this work was RFun, a 
reversible functional language. However, at the time of writing, RFun 
was very limited in its abilities, especially when dealing with 
numbers. Cryptographic code almost always uses some form of bitwise 
operations and those are not available under Peano number arithmetics; 
secondly, most of such code is also imperative, not functional. 
Furthermore, a new development of this language was just under way, so 
it made sense to stick to a more stable and workable tool.

%\section{Terminology}

%directly reversible = an injective function -- wtf??? so how's Bennett's trick related to that?


\chapter{Design}

%This chapter defines the inputs and outputs for crypto algorithms, 
%expected behaviour and goals and finally methods of evaluating those 
%properties. 

\section{Avoiding state information leakage}

Clearly, reversible code cannot delete information since the output 
would not be reversible back into its corresponding input. Similarly, 
no information can be "created", since that would be information 
deletion when reversed. Information can be duplicated and 
de-duplicated; one is the inverse of the other, so it is consistent 
with reversibility. In theory, we can apply this in our bijective 
crypto algorithms so that for every bit of input information, we output 
exactly one bit of output information. So assuming the key and 
plaintext memory locations are taken care of properly with regards to 
security, then any implementation can avoid leaking information by 
using only these safe memory locations and uncalculating anything else.

\section{Example algorithms}

Let us select some algorithms to work with. We will start with one of 
the simplest and still practical encryption algorithms, then move on to 
some more well known.

Note that these algorithms fit in the \textbf{ARX} group as they are 
based on three operations: \textbf{A}ddition, \textbf{R}otation and 
\textbf{X}OR operation. This makes them somewhat similar, but they are 
still different enough to validate our choices; from very simple to 
complex, block and stream ciphers, old and new. The advantage is that 
all these three operations are trivially reversible.

\subsection{TEA}

The \term{Tiny Encryption Algorithm} (TEA)\cite{tea95} is, per its 
name, incredibly simple and small. It is so simple that we can even 
show the complete code for it here; however, since decryption is just a 
reversal of encryption, we are showing only the encryption procedure:

\begin{lstlisting}[language=C]
void encrypt (uint32_t* v, uint32_t* k) {
    uint32_t v0=v[0], v1=v[1], sum=0, i;           /* set up */
    uint32_t delta=0x9e3779b9;                     /* a key schedule constant */
    uint32_t k0=k[0], k1=k[1], k2=k[2], k3=k[3];   /* cache key */
    for (i=0; i < 32; i++) {                       /* basic cycle start */
        sum += delta;
        v0 += ((v1<<4) + k0) ^ (v1 + sum) ^ ((v1>>5) + k1);
        v1 += ((v0<<4) + k2) ^ (v0 + sum) ^ ((v0>>5) + k3);
    }                                              /* end cycle */
    v[0]=v0; v[1]=v1;
}
\end{lstlisting}
\textit{\footnotesize Source: Wikipedia}\\

The decryption procedure is analogous. Both procedures take two 
arguments, a key and some data, and perform the necessary operations; 
if we avoided using the local variables \code{v0}, \code{v1}, \code{k0} 
and \code{k1} and replaced them by what they actually refer to, \ie 
\code{v[0]}, \code{v[1]}, \code{k[0]} and \code{k[1]}, the changes 
would be performed in-place, thus avoiding spilling any secrets 
anywhere else in memory, except for processor registers.

There are two subsequent extensions, named \term{XTEA} and 
\term{XXTEA}, which improve the cryptographic security. XTEA performs 
slightly different updates of those \code{v*} variables, but is the 
same otherwise. XXTEA finally encrypts a whole block of data instead of 
just two words. It also uses a few more local variables, but the 
general approach is identical. We discuss XXTEA in more detail in 
section \ref{sec:impl:tea}.

\subsection{RC5}

A somewhat more complicated algorithm is \term{RC5}, which was a 
candidate for the \term{AES}. Unlike TEA, RC5 already performs 
something called \term{key expansion}. Key expansion often extends the 
key to a larger data stream; in the case of RC5, the number of 
encryption rounds is the defining factor in that length. Additionally, 
some further mathematical operations are applied to it, increasing 
randomization of key bit distribution. This expansion is performed once 
for a single key and then re-used, since it only depends on the key and 
some predefined constants. The encryption itself can then be much 
simpler, as is the case here. Let us look at RC5 more closely:

\begin{lstlisting}[language=C]
void RC5_ENCRYPT(uint32_t *v, uint32_t *S, int rounds)
{
   uint32_t i, A = v[0] + S[0], B = v[1] + S[1];
   
   for(i = 1; i <= r; i++)
   {
      A = ROTL(A ^ B, B) + S[2*i];
      B = ROTL(B ^ A, A) + S[2*i + 1];
   }
   v[0] = A; v[1] = B;
}
\end{lstlisting}
\textit{\footnotesize Sources: Wikipedia, RC5 paper}\\

Just like with TEA, the encryption can be performed in-place. Unlike 
TEA, the procedure technically asks for a third parameter, a number of 
rounds; of course that can be removed by using a fixed constant. Also 
note that the \code{S} variable represents the expanded key, which 
brings us to the expansion itself, which is finally interesting 
(presenting it in a pseudocode for clarity):

\begin{lstlisting}[language=Python]
# w - The length of a word in bits, typically 16, 32 or 64.
# u - The length of a word in bytes.
# b - The length of the key in bytes.
# K[] - The key, considered as an array of bytes (using 0-based indexing).
# c - The length of the key in words (or 1, if b = 0).
# L[] - A temporary working array used during key scheduling.
# r - The number of rounds to use when encrypting data.
# t = 2(r+1) - the number of round subkeys required.
# S[] - The round subkey words.

# Break K into words
u = w / 8
c = ceiling( max(b, 1) / u )
# L is initially a c-length list of 0-valued w-length words
for i = b-1 down to 0 do:
    L[i/u] = (L[i/u] << 8) + K[i]
     
# Initialize key-independent pseudorandom S array
# S is initially a t=2(r+1) length list of undefined w-length words
S[0] = P_w
for i = 1 to t-1 do:
    S[i] = S[i-1] + Q_w
    
# The main key scheduling loop
i = j = 0
A = B = 0
do 3 * max(t, c) times:
    A = S[i] = (S[i] + A + B) <<< 3
    B = L[j] = (L[j] + A + B) <<< (A + B)
    i = (i + 1) % t
    j = (j + 1) % c

# return S
\end{lstlisting}
\textit{\footnotesize Source: Wikipedia}\\

The first loop only makes sure the key is represented in little-endian 
form. The second loop is just an initialization of the buffer 
containing the expanded key. The important randomization occurs in the 
last loop, which seems slightly complicated at first glance. The 
iteration itself does not use a single variable, but instead uses both 
\code{i} and \code{j}, since both are used as an index for different 
arrays that may have different lengths. Furthermore, the number of 
loops is also not set by a single value - it is selected as a maximum 
of two values. And last but not least, the \code{A} and \code{B} 
variables are being re-set on every loop. No wonder that when we look 
at the paper establishing RC5, we find this note: 

\textit{,,The key-expansion function has a certain amount of 
``one-wayness'': it is not so easy to determine K from S.''}

For our purpose, which is re-writing the procedure in a reversible 
language, that does not sound encouraging. On the other hand, we know that 
we can make anything reversible using a Bennett's trick at worst. The 
solution is simple: these statements are actually not 
mutually-exclusive: the fact is that S is not the only result of the 
expansion function, because both S and L get updated so that they are 
dependent on each other. So it is true that getting K from S is not 
easy, but it is easy to get K from both S and L.

\subsection{Salsa20 and Chacha20}

Salsa20 and Chacha20 are relatively new algorithms. Unlike previous 
algorithms, they are \term{stream ciphers}, meaning they do not take 
the plaintext as an argument, but they generate a stream of random 
bits, which is then combined (\eg using XOR) with the plaintext. 
Chacha20 is based on Salsa20 and has improved both performance and 
security. Code complexity is comparable to RC5, but it is more synoptic 
with Chacha20 being even more so than Salsa20 - this becomes clear when 
reading the Janus code.

Interestingly, both documents for Salsa20 and Chacha20 mention 
invertibility unlike other documents. Invertibility referred to the 
mathematical property of bijection rather than reversibility as in 
our case. Even though invertibility is explicitly mentioned (and 
required), the code is not reversible directly, as they require some 
manipulation through a temporary local variable. That is caused by the 
lack of proper feature of the Janus language rather than some deeper 
reason. Details, including a suggestion for improvement of Janus, are 
discussed in section \ref{sec:impl:salsa}.

\subsection{Simon and Speck}

Finally we discuss two ciphers designed by NSA, the American 
intelligence agency. We wanted to see whether reversibility has any 
potential to reveal crypto algorithm weaknesses. Nothing specific was 
discovered except a general sense of difficulty implementing them. 
Although Simon is simpler than its companion, Speck contains multiple 
uses of Bennett's tricks due to complications with reversing some of 
the calculations. Even when compared to Salsa20 and Chacha20, which 
both have more lines of code than Speck, both were easier to write. 
That experience may be explained with more precise documentation. The 
fact that both Salsa20 and Chacha20 contain fewer and \textit{,,less 
weird''}~\footnote{From purely personal experience. I cannot offer 
formal quantification.} occurrences of Bennett's tricks, however, 
cannot.

\begin{lstlisting}
\end{lstlisting}


\section{Defining inputs, outputs and expected behaviour}

Thanks to assumed bijection, we can define two inputs and two outputs for each of our algorithms. \todo{even RC5?}
\todo{ Also stream ciphers, watch out}

\section{Verification methods}

To verify whether our code fits our formal expectations, we have 
several options to choose from. We will split them into two groups 
based on user involvement and discuss these separately.

\subsection{Manual methods}

Among the simplest, most demanding and least dependable is experimental 
behaviour observation and/or testing. We will experimentally show our 
code cleans up memory after usage.

The second way is showing that cleanup processing using a debugger. 
That way we can directly point to location in the code where that 
cleanup occurs. We will use \term{gdb} to demonstrate memory getting 
zeroed out.

The third way is manually analysing the actual generated machine code. 
We will isolate and analyse the instructions related to memory cleanup.

\subsection{Automatic methods}
\label{sec:verifmet}

There are some ways to verify different properties automatically; or to 
enforce them. Methods like theorem provers, however, tend to be 
difficult to employ. They often require non-trivial amount of skilled 
work just to formally prove correctness can be as high or even higher 
than implementing the algorithm in the first place. \todo{citation 
needed} We will not be using this method.

On a related note, there has been some research made into and 
real-world application of language-based safe code recently: notably, 
\term{F*} and its derivative \term{Low*} \cite{Low*}, and the related 
language \term{Vale}\cite{vale2017}. Low* has been used in the 
\term{HACL} project, which has been integrated into Mozilla Firefox as 
a crypto library. Vale is a language dedicated to writing verifiable 
low-level code; its authors also implemented some crypto algorithms, 
successfully verified their resistance to state-based and timing-based 
information leakage and even showed some performance gains. Vale has a 
huge potential, but at the time of writing it also lacks stability, 
documentation, examples and manuals. Only an unsuccessful attempt to 
use Vale was made. Details will be discussed later. \todo{discuss later}

Another approach is turning the compiler into an ally rather an 
obstacle that has to be worked-around. Such approach was taken by the 
authors in a very recent paper\cite{whatyouc}. They have implemented an 
extension to the LLVM/Clang compiler called \term{zerostack} that adds 
the ability for clearing out registers and stack, and to make a 
selection between two values in constant time. The C11 standard also 
added \code{memset\_s}, which the compiler is supposed to guarantee to 
never remove and so it can be used for clearing out heap memory. We 
will be using this method in combination with the manual methods to 
verify results.

Lastly, there are projects that offer tracking of sensitive data (or 
rather taints\todo{taints? is this even the right word?}) when moved 
around in memory. These are \term{taintgrind} and its derivative 
\term{secretgrind}, with the latter being specifically designed for 
crypto algorithms. We will be using secretgrind to track tainted data. 
This will be in combination with the register and stack clearing 
method, described above, to make sure nothing of the secret computation 
is preserved.



\chapter{Implementation}

In this chapter we show our Janus code for the selected algorithms and 
discuss it. We talk about the conversion from irreversible to 
reversible and even come up with improvements to Jana, the Janus 
interpreter.


\section{Algorithms}

\subsection{TEA}
\label{sec:impl:tea}

TEA translates to Janus almost directly like in any language, only the 
local variable allocation and deallocation is Janus-specific.

\begin{lstlisting}[language=Janus]
procedure TEA_encipher(u32 data[], u32 key[])
    local u32 delta = 2654435769
    local u32 sum = 0
    iterate int i = 1 by 1 to 32
        sum += delta
        data[0] += ((data[1] * 16) + key[0]) ^
                    (data[1] + sum) ^
                    ((data[1] / 32) + key[1])
        data[1] += ((data[0] * 16) + key[2]) ^
                    (data[0] + sum) ^
                    ((data[0] / 32) + key[3])
    end
    delocal u32 sum = 32*delta
    delocal u32 delta = 2654435769
\end{lstlisting}

XTEA is just as easy to translate. With XXTEA, we need to carefully 
update some local variables. Let us look at the code, starting with C:

\begin{lstlisting}
#define MX ((z>>5^y<<2) + (y>>3^z<<4) ^ (sum^y) + (k[p&3^e]^z))

long xxtea(unsigned long* v, unsigned long n, long* k) {
  unsigned long z = v[n-1], y = v[0], sum = 0, e, delta = 0x9e3779b9;
  long p, q;
  q = 6 + 52/n;
  while (q-- > 0) {
    sum += delta;
    e = (sum >> 2) & 3;
    for (p=0; p<n-1; p++) {
      y = v[p+1];
      z = v[p] += MX;
    }
    y = v[0];
    z = v[n-1] += MX;
  }
  return 0;
}
\end{lstlisting}
\textit{\footnotesize Source: Wikipedia, edited}\\

Note that the code is readable thanks to the MX macro, which is not 
available with Janus\footnote{We could use some preprocessor like 
\term{m4} or \term{cpp} like C does, but the Janus interpreter would 
not translate that into C without recognizing (or ignoring) it 
itself. It is, therefore, a good suggestion for Jana improvement.}.

The problematic parts are the \code{e}, \code{y} and \code{z} 
variables. They are being reset repeatedly, erasing information. 
Furthermore, \code{z} is being set to a value that is dependent on 
itself and that is not allowed in Janus. Since \code{e} is set to 
values dependent only on the variable \code{sum}, which is altered only 
at the beginning of the main loop, and then some constants, we know we 
can recalculate \code{e} at the end of the loop again. So to update 
\code{e} reversibly, we simply subtract the same value it has at the 
end of the loop.

The problem with \code{y} and \code{z} is their dependency on each 
other\footnote{This dependency is eventual with \code{y} and immediate 
with \code{z}.}. The easy way to overcome this is to use a Bennett's 
trick. The harder way is figuring out what is actually being updated 
and with what values. In the case of XXTEA this is still fairly 
manageable, so let us practice. The first thing we realize is that we 
can safely replace \code{y} with whatever data it was set to. We see 
that it is being set to the value of \code{v[p+1]}, so we replace all 
occurrences of y with that. Along with that we also have to partially 
unroll the inner loop to make sure that the indexed access is within 
bounds. Then, when we remove \code{y}, we notice that we don't have to 
set \code{z} as nothing but the \code{MX} macro depends on it, and in 
that case it simply has the value of the data at the previous index 
\code{p}. Again, we replace all occurrences of \code{z} with 
\code{v[p-1]} and pay attention to array bounds.

Finally, the variable \code{p} also needs care when resetting, but that 
is straightforward to do according to the inner loop exit.

The resulting Janus code follows the above description:

\begin{lstlisting}
procedure XXTEA_encipher(u32 data[], u32 key[])
    local u32 delta = 2654435769
    local int n = 4 //size(data)
    local u32 sum = 0
    local u32 e = 0
    local int p = 1
    local int q = 6 + 52/n
    from q = 6 + 52/n loop
        sum += delta
        e += (sum / 4) & 3
        data[0] += ((((data[n-1] / 32) ^ (data[1] * 4)) +
                     ((data[1] / 8) ^ (data[n-1] * 16))) ^ 
                    ((sum ^ data[1]) + (key[e] ^ data[n-1])))
        from p = 1 loop
            data[p] += ((((data[p-1] / 32) ^ (data[p+1] * 4)) + 
                         ((data[p+1] / 8) ^ (data[p-1] * 16))) ^
                        ((sum ^ data[p+1]) + (key[(p & 3) ^ (int) e] ^ data[p-1])))
            p += 1
        until p = n-1
        data[n-1] += ((((data[n-2] / 32) ^ (data[0] * 4)) +
                       ((data[0] / 8) ^ (data[n-2] * 16))) ^
                      ((sum ^ data[0]) + (key[(p & 3) ^ (int) e] ^ data[n-2])))
        p -= (n-2)
        e -= (sum / 4) & 3
        q -= 1
    until q = 0
    delocal int q = 0
    delocal int p = 1
    delocal u32 e = 0
    delocal u32 sum = (6+52 / (u32)n)*delta
    delocal int n = 4 //size(data)
    delocal u32 delta = 2654435769
\end{lstlisting}

\subsection{RC5}

Translating RC5 into Janus was difficult and required extra work in 
designing the reversible code. The encryption function is simple, but 
the key expansion function has grown considerably during translation 
even when partially altered - the last loop has a simplified exit 
condition.

\todo{polish RC5. L is not initialized because of the missing first 
loop. encryption function has wrong api, it should do changes in-place}

\begin{lstlisting}
\end{lstlisting}

\subsection{Salsa20 and Chacha20}
\label{sec:impl:salsa}

In case of Salsa20, we are not taking existing C code or pseudocode and 
translating it into Janus. We are following a precise mathematical 
formulation that is presented in the specification\cite{salsa}. As a 
result, we do not compare our implementation with C code anymore. We 
focus solely on Janus instead. We also focus on the difficult or 
annoying parts of the implementation by suggesting enhancements to 
Janus.

\paragraph{First enhancement}

The first issue appeared with the \code{quarterround} function. Initially, a 
misunderstanding of the specification resulted in applying the second 
Bennett's trick like this:

\begin{lstlisting}[language=Janus]
procedure quarterround(u32 seq[4])
	local u32 tmp_seq[4]
	call quarterround_bt(seq, tmp_seq)
	seq[0] <=> tmp_seq[0]
	seq[1] <=> tmp_seq[1]
	seq[2] <=> tmp_seq[2]
	seq[3] <=> tmp_seq[3]
	uncall quarterround_bt(seq, tmp_seq) // 2nd Bennett's trick
	delocal u32 tmp_seq[4]
\end{lstlisting}

Noticing the error, the \code{quarterround} procedure was reimplemented 
to make its computations in-place. Now only one temporary variable is 
required, and that is only due to the feature lacking within the Janus 
language. The suggestion is to implement functions, \ie procedures that 
return a value. Let us consider this extension with an example:

\begin{lstlisting}[language=Janus]
// classic Janus
procedure multiply_proc(int a, int b, int c)
  c += a * b

// Proposed extension
function multiply_func(int a, int b)
  return a * b

// Usage of the procedure, computes result += a * b * c
procedure multiply_three(int a, int b, int c, int result)
  local int tmp = 0
  call multiply_proc(a, b, tmp)
  call multiply_proc(tmp, c, result)
  uncall multiply_proc(a, b, tmp)
  delocal int tmp = 0


// Usage of the function
procedure multiply_three(int a, int b, int c, int result)
  result += call multiply_func(call multiply_func(a, b), c)
\end{lstlisting}

Obviously the procedure \code{multiply\_three} would also be a function, 
but we disregard that for this example.

These functions would be just a syntactic sugar compared to what the 
programmers does with procedures. The extension then would follow these 
steps:

\begin{enumerate}
\item automatically allocate a hidden temporary variable, local to the caller, for storing the returned value
\item add that variable as another parameter
\item replace the \code{return} statement with adding the value to the new parameter (using the \code{+=} operator)
\item add corresponding uncall for clearing that hidden variable in the caller
\end{enumerate}

Note that with these steps followed, the syntactic sugar would in fact 
unroll into this:

\begin{lstlisting}[language=Janus]
// Usage of the procedure, computes result += a * b * c
procedure multiply_three(int a, int b, int c, int result)
  local int hidden_tmp1 = 0
  local int hidden_tmp2 = 0
  call multiply_proc(a, b, hidden_tmp1)
  call multiply_proc(hidden_tmp1, c, hidden_tmp2)
  result += hidden_tmp2
  uncall multiply_proc(hidden_tmp1, c, hidden_tmp2)
  uncall multiply_proc(a, b, tmp)
  delocal int hidden_tmp2 = 0
  delocal int hidden_tmp1 = 0
\end{lstlisting}

This code is not as efficient as the hand-written equivalent above, 
however, in the case of the \code{quarterround} function in Salsa20, 
there would be no performance loss. Consider this piece of code, 
with both the hand-written form and the one with the suggested 
extension:

\begin{lstlisting}[language=Janus]
  // z1 = y1 ^ ((y0 + y3) <<< 7)
  tmp += seq[0] + seq[3]
  call rotate_left_u32(tmp, 7)
  seq[1] ^= tmp
  uncall rotate_left_u32(tmp, 7)
  tmp -= (seq[0] + seq[3])
  
  // z1 = y1 ^ ((y0 + y3) <<< 7)
  seq[1] ^= call rotate_left_u32(seq[0] + seq[3], 7)
\end{lstlisting}

Following the steps above, the automatically unrolled code would be the 
same as the hand-written code. This extension would allow for replacing 
the 22-line procedure with a 5-line function with the same semantics 
and much better readability.

\paragraph{Second enhancement}

The obvious second hassle is the way we exchange values between 
a temporary array and the original array in the cases of the 
\code{doubleround} (only Chacha20), and \code{columnround} and 
\code{rowround} (only Salsa20) procedures. This is only caused by the 
need to access different elements in an array. This is the current form 
of the code:

\begin{lstlisting}[language=Janus]
procedure rowround(u32 seq[16])
	local u32 tmp_row[4]
	...
	// third row
	tmp_row[0] <=> seq[10]
	tmp_row[1] <=> seq[11]
	tmp_row[2] <=> seq[8]
	tmp_row[3] <=> seq[9]
	call quarterround(tmp_row)
	tmp_row[0] <=> seq[10]
	tmp_row[1] <=> seq[11]
	tmp_row[2] <=> seq[8]
	tmp_row[3] <=> seq[9]
	...
	delocal u32 tmp_row[4]
\end{lstlisting}

With this example (and considering the rest of the procedure) we see 
the need to access 4 elements, in two consecutive ranges. In the next 
listing we are accessing columns also in two ranges:

\begin{lstlisting}[language=Janus]
procedure columnround(u32 seq[16])
	local u32 tmp_col[4]
	...
	// second row
	tmp_col[0] <=> seq[5]
	tmp_col[1] <=> seq[9]
	tmp_col[2] <=> seq[13]
	tmp_col[3] <=> seq[1]
	...
	delocal u32 tmp_col[4]
\end{lstlisting}

We have taken a different approach with Chacha20's \code{doubleround} 
procedure. We changed the \code{quarterround} procedure signature to 
take the 4 elements as single arguments instead of as a single array. 
This simplified the \code{doubleround} procedure significantly:

\begin{lstlisting}[language=Janus]
procedure doubleround(u32 seq[16])
	...
	// third diagonal
	call quarterround(seq[2], seq[7], seq[8], seq[13])
	...
\end{lstlisting}

Notice that there is no need for any temporary variable. This approach 
works, but, to a programmer's eye, it feels too unwieldy. In many other 
languages, the solution to that is called \term{array slicing}. That is 
a technique that allows splitting, cutting or \term{slicing} parts the 
array into a new array. This new array is often not a full array with 
its own copies of the elements, but only a syntactic sugar, just a 
mapping to the original array. If we wanted to slice the array 
according to the Chacha20's \code{doubleround} indexing, we would need 
either a powerful expression system to describe the complex slices (the 
diagonals), or a way to pick the elements manually.

%\begin{lstlisting}[language=Janus]
%procedure doubleround(u32 seq[16])
	%...
	%// first diagonal
	%call quarterround(seq[0], seq[5], seq[10], seq[15])
	%// second diagonal
	%call quarterround(seq[1], seq[6], seq[11], seq[12])
	%// third diagonal
	%call quarterround(seq[2], seq[7], seq[8], seq[13])
	%2             % 4  = 2
	%2 + 5         % 8  = 7
	%2 + 5 + 5     % 12 = 12 ... - 4 = 8  //bullshit, need to iterate over 4s I think... 2+4+1+4+1 ?
	%2 + 5 + 5 + 5 % 16 = 17 ... - 4 = 13
	%call quarterround(seq[(0..3)*4 + (0..3)*5 + 2])
	%...
%\end{lstlisting}


\begin{lstlisting}[language=Janus]
procedure doubleround(u32 seq[16])
	...
	// third diagonal
	call quarterround(seq[2,7,8,13])
	...
\end{lstlisting}


\begin{lstlisting}[language=Janus]
\end{lstlisting}

\paragraph{Chacha20}

Chacha20, being only a minor update to Salsa20, does not differ 
dramatically much from its predecessor. The differences consist of 
different operation ordering and are in the \code{doubleround} and the 
\code{quarterround} procedures. While Salsa20 has its 
\code{columnround} and \code{rowround} functions, their code is mixed 
in together in the Chacha20 variant.

Worth noticing is the difference between the variants of the 
\code{quarterround} function. We have already examined the details of 
the Salsa20 variant and we noticed, that with current version of Jana, 
we do need to utilize at least one temporary local variable. We have 
also suggested enhancing Janus to include \term{functions} in addition 
to \term{procedures}. In the case of Chacha20, however, no such 
extension is necessary. Their author correctly noted that both variants 
are invertible, but from our perspective one is easier to code 
reversibly. The difference is that the Salsa20 variant uses non-trivial 
bracketed expression for each single element that is updated, and 
intermediate calculations need to store intermediate results. Chacha20 
performs updates one step at a time, each of them being used and none 
of them being temporary.

\subsection{Simon and Speck}


\section{Lack of leakage and translation}




\chapter{Evaluation}
\label{sec:eval}

In this chapter we attempt to test for the properties discussed in 
section \ref{sec:goals}. These are the goals again:

\begin{enumerate}
\item show lack of state information leakage,
\item exploit the duality of encryption and decryption in a reversible setting,
\item gain practical experience with programming in a reversible language.
\end{enumerate}

We have demonstrated reaching the last goal in previous chapter, where 
we discussed how we implemented some crypto algorithms, showed some 
examples and even gave suggestions for improving the Janus language. At 
the same time, we also reached the second goal, as we implemented only 
encryption functions. The reader can execute our programs which also 
include basic keys and data. Uncalling the encryption function is 
equivalent to calling the decryption function.

The first goal remains and we deal with it here.

\section{Design}

We already mentioned some options for verifying our claim in section 
\ref{sec:verifmet}. The best tool for the job would be Vale, but that 
turned out to be more of a trouble instead of help. There is no doubt 
Vale can be highly useful, but at the time of writing the maturity of 
the project is just not at the required level. Even the installation 
was problematic; we encountered issues that are not usually expected, 
like folder tree structure incompatibility and case sensitivity in file 
naming. There were also other, more common difficulties, like 
dependencies being required at specific version and package management 
issues. When the installation finally succeeded, the problems did not 
disappear. The biggest problems are related to the usability: there is 
very little documentation, there are practically no examples and even 
the source code seems hardly readable\footnote{Subjective assessment, 
the source code for the Vale tool is dense and not commented.}. 
Furthermore, it seems that to take full advantage of the language, the 
user also needs to understand the target language (in our case Dafny) 
and perhaps how the translation is performed. Last but not least, the 
language is extremely complicated and the effort in learning Vale and 
implementing an algorithm in it might not be any lower than just using 
a more classic language and using already established tools and methods 
for debugging and tuning.

Since we cannot use Vale, the next best option is taint tracking in 
combination with automatic cleanup of registers and stack memory. 
Ideally we could use both at the same time, but experiments have shown 
there is a clash between them. The reason is not perfectly clear, but 
suspicion lies with how secretgrind tracks \term{libc} calls and 
the fact that zerostack links with \term{musl-libc}. It should 
be possible to remove this problem in a production scenario without too 
much effort. Since secretgrind also tracks stack, though, we do 
not necessarily require this for the evaluation.

For the evaluation we suggest performing an experiment on selected 
algorithms to approach describing the reality of state leakage. Let us 
describe the experiment in steps:

\begin{description}

\item[Step 1: Translation] We start by translating Janus code into 
C/C++ code. The translation is performed by the Jana interpreter when 
invoked with the \code{-c} option. On its own, the C/C++ code can be 
compiled and executed and performs equivalently to its Janus 
counterpart. There is no input from outside the program, however.

\item[Step 2: Reference] We take a reference implementation of the same 
algorithm and adjust both the reference and the translated code to read 
and process data the same way. The reference should form a baseline for 
our comparison.

\item[Step 3: Taint tracking] We compile both programs the same way and 
input the same data. We compare their execution using 
secretgrind and evaluate their scores in number of bytes 
potentially leaked.

\item[Step 4: Zerostack substitution] Secretgrind traces the 
source of the leaks (taints), and so we can examine them. We then 
recompile the programs with zerostack and re-evaluate the 
situation.

\end{description}

The last step is somewhat problematic. During the experimentation an 
issue with the combination of secretgrind and zerostack 
appeared. To be fully effective, zerostack requires all linked 
libraries, including \code{libc}, to be also modified with cleanup 
code. The author of zerostack chose \code{musl-libc} for its 
simplicity, which reduces potential problems during compilation. 
However, it appears \code{secretgrind} does not cooperate very well 
with that library, as any attempts to trace binaries, that are linked 
with zerostack-enabled \code{libc}, results in various errors. 
At least one of them suggests secretgrind is somehow sensitive 
to the choice of \code{libc}, even though the original version, 
\term{taintgrind}, should not be\footnote{Confirmed by its author over 
e-mail.}. This means we will resort to manual examination of the 
executable binary file.

On a last note, the reason to use zerostack at all is just 
usefulness. At this time, reversible hardware is not available. 
Reversible platforms, which would simulate reversible hardware, are 
either not commonly available, or just not practically feasible for any 
serious application. For our research to be usable in the real world, 
\ie taking the advantages of reversible embedding out of the 
reversible-only world, we translate the Janus code and make sure the 
code keeps the necessary guarantees. Zerostack offers us the 
proper cleanup on every function/procedure exit, thus avoiding 
information leakage.

\section{Experiments}

\subsection{Step 1: Translation}

The code is translated using Jana. There are no manual modifications to 
make the code compile, however there are some modifications in order to 
perform the experiment. One set of modifications is done so that we 
secretgrind has tainted memory to track. A second set of 
modifications is done in compliance with zerostack, and those 
will be explained in the last step. We use the older version of 
translation due to added complexity in recent Jana versions. Originally 
Jana used a direct translation to C-style arrays, that was substituted 
by \code{std::array} for a short time and then by \code{std::vector}. 
Reason for the replacement of C-style arrays was keeping some record of 
the size of the array. Janus interpreter has a keyword \code{size}, 
which returns the size of the array, since it is always known as it 
does not change. C, on the other hand, uses simple pointers to memory 
and does not track the size of the allocated block\footnote{Not related 
to heap allocations, which do need to track that information 
internally.}. The reason to stick to the original translation is 
simplicity in tracking memory using secretgrind and the 
immaturity of the recent changes to Jana.

\subsection{Step 2: Reference}

To have something to check our code against, we download reference 
implementations from the Internet. We also add some boilerplate code to 
create executable self-standing programs instead of libraries, and we 
also add a function for reading from a file. This function uses the 
\code{read()} function, since that is simple enough to avoid confusion 
and complexity when tracked by secretgrind. For the same reason 
we also disable code that prints out the contents of arrays - prior 
experiments have shown that functions like \code{printf} and \code{scanf} manipulate and 
taint more memory.

\subsection{Step 3: Taint tracking}

\subsubsection{XXTEA}

XXTEA is the first algorithm for us to examine. While we already talked 
about TEA and XTEA, they are somewhat simpler and perform their 
operations in-place, which limits potential leakage. XXTEA, on the 
other hand, required some work to make the updates in-place. That makes 
it interesting to compare to classic irreversible implementation.

For the testing, we are using the \code{TEA.janus} file, containing all 
three TEA-family algorithms; we only comment out the TEA and XTEA 
invocations. For the reference implementation, we make minor edits to 
the version on Wikipedia\cite{xxtea-wiki}. For input, we load 
both key and data for encryption from a file, that will be marked as 
tainted by secretgrind. Note that both the key and the data 
block take up 16 bytes of memory, so a non-leaking implementation 
should see exactly 32 bytes in total of tainted memory. Let us look at 
the secretgrind output for the reference code:

\begin{lstlisting}[language=,breaklines=true]
***(2) (stack)	 range [0xffefffdcc - 0xffefffdcf] (4 bytes) is tainted
   > (stack) [0xffefffdcc - 0xffefffdcf] (4 bytes): XXTEA-C.c:51:@0xffefffdcc:y
        tainted     at 0x400ABF: xxtea_dec (XXTEA-C.c:62)
                    by 0x400C68: main (XXTEA-C.c:96)

***(1) (stack)	 range [0xffefffdd4 - 0xffefffdd7] (4 bytes) is tainted
   > (stack) [0xffefffdd4 - 0xffefffdd7] (4 bytes): XXTEA-C.c:51:@0xffefffdd4:z
        tainted     at 0x400A55: xxtea_dec (XXTEA-C.c:61)
                    by 0x400C68: main (XXTEA-C.c:96)

***(1) (stack)	 range [0xffefffe10 - 0xffefffe2f] (32 bytes) is tainted
   > (stack) [0xffefffe10 - 0xffefffe1f] (16 bytes): XXTEA-C.c:90:@0xffefffe10:key
        tainted     at 0x4F196B0: __read_nocancel (syscall-template.S:81)
                    by 0x400B63: read_data_binary (XXTEA-C.c:77)
                    by 0x400C3E: main (XXTEA-C.c:93)
   > (stack) [0xffefffe20 - 0xffefffe23] (4 bytes): XXTEA-C.c:91:@0xffefffe20:more_data
        tainted     at 0x400AB7: xxtea_dec (XXTEA-C.c:62)
                    by 0x400C68: main (XXTEA-C.c:96)
   > (stack) [0xffefffe24 - 0xffefffe27] (4 bytes): XXTEA-C.c:91:@0xffefffe24:more_data[1]
        tainted     at 0x400A29: xxtea_dec (XXTEA-C.c:59)
                    by 0x400C68: main (XXTEA-C.c:96)
   > (stack) [0xffefffe28 - 0xffefffe2b] (4 bytes): XXTEA-C.c:91:@0xffefffe28:more_data[2]
        tainted     at 0x400A29: xxtea_dec (XXTEA-C.c:59)
                    by 0x400C68: main (XXTEA-C.c:96)
   > (stack) [0xffefffe2c - 0xffefffe2f] (4 bytes): XXTEA-C.c:91:@0xffefffe2c:more_data[3]
        tainted     at 0x400A29: xxtea_dec (XXTEA-C.c:59)
                    by 0x400C68: main (XXTEA-C.c:96)

Total bytes tainted: 40
\end{lstlisting}

The printout details three stack ranges. The first two are caused by 
the \code{y} and \code{z} variables, while the last one makes up the 
actual input for the algorithm, that is the key and the data. The leak 
is therefore 8 bytes over the expected 32 bytes.

Because our Janus implementation had to avoid these local erasable 
variables, and thus perform the updates in-place, there are no extra 
leaks:

\begin{lstlisting}[language=,breaklines=true]
***(1) (stack)	 range [0xffefffe20 - 0xffefffe3f] (32 bytes) is tainted
   > (stack) [0xffefffe20 - 0xffefffe2f] (16 bytes): TEA-translated.cpp:150:@0xffefffe20:key
        tainted     at 0x4F196B0: __read_nocancel (syscall-template.S:81)
                    by 0x4015F6: read_data_binary(unsigned int*, unsigned long, unsigned int*, unsigned long) (TEA-translated.cpp:139)
                    by 0x4016F9: main (TEA-translated.cpp:167)
   > (stack) [0xffefffe30 - 0xffefffe33] (4 bytes): TEA-translated.cpp:152:@0xffefffe30:more_data
        tainted     at 0x401450: XXTEA_encipher_reverse(unsigned int*, unsigned int*) (TEA-translated.cpp:117)
                    by 0x40171F: main (TEA-translated.cpp:180)
   > (stack) [0xffefffe34 - 0xffefffe37] (4 bytes): TEA-translated.cpp:152:@0xffefffe34:more_data[1]
        tainted     at 0x40137F: XXTEA_encipher_reverse(unsigned int*, unsigned int*) (TEA-translated.cpp:114)
                    by 0x40171F: main (TEA-translated.cpp:180)
   > (stack) [0xffefffe38 - 0xffefffe3b] (4 bytes): TEA-translated.cpp:152:@0xffefffe38:more_data[2]
        tainted     at 0x40137F: XXTEA_encipher_reverse(unsigned int*, unsigned int*) (TEA-translated.cpp:114)
                    by 0x40171F: main (TEA-translated.cpp:180)
   > (stack) [0xffefffe3c - 0xffefffe3f] (4 bytes): TEA-translated.cpp:152:@0xffefffe3c:more_data[3]
        tainted     at 0x40124E: XXTEA_encipher_reverse(unsigned int*, unsigned int*) (TEA-translated.cpp:110)
                    by 0x40171F: main (TEA-translated.cpp:180)

Total bytes tainted: 32
\end{lstlisting}

It has to be pointed out that the leakage-free result could be achieved 
the same way originally. The difference is that the opposite is not 
true: reversible programming does not allow the leaking version. Still 
this is a fairly simple algorithm, so let us move on.

\subsubsection{Chacha20}

A second algorithm we will be looking at is Chacha20. The nice thing 
about this algorithm is that its author also wrote the reference 
implementation along with several optimized versions\footnote{All of 
them can be found on their author's website.}. For simplicity and 
fairness we are comparing only the most generic reference version.

We are tracking only the plaintext part of the input. Expected number 
of bytes tainted therefore is $16\cdot4 = 64$.

%In the reference version, there are 7 stack ranges in total with far 
%too many individual locations to show them. We only show an excerpt 
%from the output with the total number of bytes tainted at the bottom:

%\begin{verbatim}
%***(1) (stack)	 range [0xffefffbb0 - 0xffefffbef]	 (64 bytes)	 is tainted
%***(2) (stack)	 range [0xffefffc00 - 0xffefffc03]	 (4 bytes)	 is tainted
%***(1) (stack)	 range [0xffefffc30 - 0xffefffc77]	 (72 bytes)	 is tainted
%***(2) (stack)	 range [0xffefffc90 - 0xffefffc93]	 (4 bytes)	 is tainted
%***(1) (stack)	 range [0xffefffce0 - 0xffefffcff]	 (32 bytes)	 is tainted
%***(1) (stack)	 range [0xffefffd20 - 0xffefffd2f]	 (16 bytes)	 is tainted
%***(1) (stack)	 range [0xffefffd40 - 0xffefffdff]	 (192 bytes)	 is tainted
%Total bytes tainted: 384
%\end{verbatim}

In the reference version, there is one stack range for all the data. We 
only show an excerpt from the output with the total number of bytes 
tainted at the bottom:

\begin{lstlisting}[language=,breaklines=true]
***(1) (stack)	 range [0xffefffd40 - 0xffefffdff] (192 bytes) is tainted
Total bytes tainted: 192
\end{lstlisting}

Our Janus-to-C translated version is more modest: only the expected 64 
bytes are marked as tainted. For some reason, secretgrind split 
the data array into 3 sections, but on inspection they do reside right 
next each other in memory.

\begin{lstlisting}[language=,breaklines=true]
***(1) (stack)	 range [0xffefffdc0 - 0xffefffdff] (64 bytes) is tainted
   > (stack) [0xffefffdc0 - 0xffefffdc3] (4 bytes): Chacha20.janus.3.cpp:702:@0xffefffdc0:seq
        tainted     at 0x403743: Chacha20_encrypt_forward(unsigned int*, unsigned int*) (Chacha20.janus.3.cpp:637)
                    by 0x403C31: main (Chacha20.janus.3.cpp:726)
   > (stack) [0xffefffdc4 - 0xffefffdc7] (4 bytes): Chacha20.janus.3.cpp:702:@0xffefffdc4:seq[1]
        tainted     at 0x403743: Chacha20_encrypt_forward(unsigned int*, unsigned int*) (Chacha20.janus.3.cpp:637)
                    by 0x403C03: main (Chacha20.janus.3.cpp:723)
   > (stack) [0xffefffdc8 - 0xffefffdff] (56 bytes): Chacha20.janus.3.cpp:702:@0xffefffdc0:seq
        tainted     at 0x4F196B0: __read_nocancel (syscall-template.S:81)
                    by 0x403A05: read_data_binary(unsigned int*, unsigned long, unsigned int*, unsigned long) (Chacha20.janus.3.cpp:686)
                    by 0x403B78: main (Chacha20.janus.3.cpp:711)

Total bytes tainted: 64
\end{lstlisting}

This result is a good demonstration of why our work is important: even 
the author of the crypto algorithm can and will make a mistake when 
writing the code. Admittedly the author may not have even considered 
the problem and therefore have not made any attempts for avoiding it, 
but that is still a valid argument. Such code is still vulnerable no 
matter the intention.


\subsection{Step 4: Zerostack substitution}

Zerostack offers three types of operation, differing by what 
they are based on: \term{function}, \term{stack} and finally 
\term{call-graph}. We tested all three and found the stack-based 
operation non-operational due to a crash in the LLVM compiler. The 
first one, function-based solution, is the simplest one but also 
reportedly\cite{whatyouc} the most performance-hindering one. The 
call-graph-based type of operation is the most efficient one, truly 
erasing only what is necessary. It is also the most complex one. 
Details of their implementation is left to their original 
paper\cite{whatyouc}, but we at least look at the resulting assembly 
code, which will become useful later.

\paragraph{Manual modifications}

First we make some modifications so that all our compilation passes 
execute correctly:

\begin{itemize}

\item We comment out the XTEA-* and XXTEA-* functions because we do not 
need them for this part of the experiment.

\item We change the return value of the TEA-* functions from 
\code{void} to \code{int} and append \code{"return 0;"} at the end of 
each of them. This is only so that the patching process for the 
call-graph method works. The patching script also required some fixing: 
we had to add assembler code that zeroes out the \code{r8d} register 
using the \code{xor} instruction.

\begin{lstlisting}[language=]
diff --git a/examples/fpointer/patchme.py b/examples/fpointer/patchme.py
index 680d58f..a06ac95 100755
--- a/examples/fpointer/patchme.py
+++ b/examples/fpointer/patchme.py
@@ -699,7 +699,8 @@ class Dispatcher:
                      "EAX"   :       "\x31\xC0",
                      "EDX"   :       "\x31\xD2",
                      "ESI"   :       "\x31\xF6",
                      

+                     "R8D"   :       "\x45\x31\xC0", 
                      "R9D"   :       "\x45\x31\xC9",
                      "R10D"  :       "\x45\x31\xD2",
                      # 16bit register		
\end{lstlisting}

\item We define a macro for defining sensitive functions, which is used 
by the call-graph method. We then use that macro to annotate our two 
TEA-* functions.

\begin{lstlisting}[language=C]
#define __attr_zerostack __attribute__((annotate("SENSITIVE")))

__attr_zerostack int TEA_encipher_forward(unsigned int *data,
                                          unsigned int *key);
__attr_zerostack int TEA_encipher_reverse(unsigned int *data,
                                          unsigned int *key);
\end{lstlisting}

\item We comment out the \code{assert} checks at the end of the TEA-* 
functions. This is also due to a problem with compilation with 
zerostack enabled.

\end{itemize}


\paragraph{Compiling and analyzing}

Starting with a normally compiled\footnote{In all the shown cases we 
used the \code{-O0} option to disable optimizations.} binary, we get a 
normal function epilogue (in this case, it is the \code{TEA\_encipher} 
function). The output depends on what compiler we use. The output of 
\code{g++} looks like this:


\begin{verbatim}
  400823:   b8 00 00 00 00  mov    $0x0,%eax
  400828:   5d              pop    %rbp
  400829:   c3              retq
\end{verbatim}

The first instruction sets the register \code{eax} to zero, which is 
then used as the return value. The second one resets the stack frame 
pointer in register \code{rbp} and the \code{retq} instruction performs 
the return from the function.

LLVM's \code{clang} outputs this:

\begin{verbatim}
  400840:   31 c0                 xor    %eax,%eax
  400842:   5d                    pop    %rbp
  400843:   c3                    retq   
  400844:   66 66 66 2e 0f 1f 84  data16 data16 nopw %cs:0x0(%rax,%rax,1)
  40084b:   00 00 00 00 00 
\end{verbatim}

Similarly to what g++ produces, we start by clearing \code{eax}, 
continue with resetting stack frame pointer and exit with a 
\code{retq}. The last line contains a multi-byte \code{nop} instruction 
and only serves as a padding. That is also produced by gcc, but only 
with some optimization enabled.


Now we utilize the function-based cleanup:

\begin{verbatim}
  400860:   31 c0                 xor    %eax,%eax
  400862:   5d                    pop    %rbp
  400863:   50                    push   %rax
  400864:   48 31 c0              xor    %rax,%rax
  400867:   b9 45 00 00 00        mov    $0x45,%ecx
  40086c:   48 8d bc 24 d8 fd ff  lea    -0x228(%rsp),%rdi
  400873:   ff 
  400874:   f3 48 ab              rep stos %rax,%es:(%rdi)
  400877:   58                    pop    %rax
  400878:   48 c7 44 24 f8 fc ff  movq   $0xfffffffffffffffc,-0x8(%rsp)
  40087f:   ff ff 
  400881:   48 31 c9              xor    %rcx,%rcx
  400884:   31 d2                 xor    %edx,%edx
  400886:   89 c0                 mov    %eax,%eax
  400888:   c3                    retq   
  400889:   0f 1f 80 00 00 00 00  nopl   0x0(%rax)
\end{verbatim}

The original instructions are still there, but just before the function 
return we see 10 instructions which perform the cleanup. There are 3 
\code{xor} instructions that take a single register twice, thereby 
clearing it. The rest is slightly chaotic. \code{rax} gets pushed to 
stack for later, then cleared. The three following instructions, 
\code{mov}, \code{lea} and \code{rep stos} clear the stack by setting a 
counter, setting the starting address and repeatedly writing zeroes, 
respectively\footnote{0x45 * 8 = 0x228, \ie it writes 0x45-times 8 
bytes to overwrite the whole stack @[\%rsp-0x228, \%rsp].}. Then we 
reload \code{rax} from stack and overwrite that position on stack with 
-4\footnote{Reason for that value is unknown. Reading the source 
revealed the intent of overwriting the pushed register, but no 
explanation for that value.}. Finally, the strange-looking \code{mov 
\%eax,\%eax} instruction clears the higher 32 bits of the \code{rax} 
register.
Clearly this function epilogue correctly clears all registers and used 
stack so it is usable for our purposes.

Now the last option we consider is the call-graph-based approach, which 
is performed in two phases, where the first one is the compilation 
itself. The second one is a patching process, which we already 
mentioned in relation to this method. The first phase generates most of 
the final binary code, but the epilogue contains a placeholder for the 
real cleanup. It also generates a file with information based on the 
call graph built from the source. The second phase then reads that 
information and replaces the placeholder with proper epilogue cleanup. 
There are technical reasons for this division, mainly based on the 
shortcomings of \code{clang} compiler, but those are not important here.

Below is the unpatched epilogue with the placeholder cleanup:

\begin{verbatim}
  4006b0:   31 c0                 xor    %eax,%eax
  4006b2:   5d                    pop    %rbp
  4006b3:   50                    push   %rax
  4006b4:   48 31 c0              xor    %rax,%rax
  4006b7:   b9 67 45 23 01        mov    $0x1234567,%ecx
  4006bc:   48 8d bc 24 f0 cd ab  lea    -0x76543210(%rsp),%rdi
  4006c3:   89 
  4006c4:   f3 48 ab              rep stos %rax,%es:(%rdi)
  4006c7:   58                    pop    %rax
  4006c8:   48 c7 44 24 f8 fc ff  movq   $0xfffffffffffffffc,-0x8(%rsp)
  4006cf:   ff ff 
  4006d1:   66 66 90              data16 xchg %ax,%ax
  4006d4:   66 66 90              data16 xchg %ax,%ax
  4006d7:   66 66 90              data16 xchg %ax,%ax
  4006da:   66 66 90              data16 xchg %ax,%ax
  4006dd:   66 66 90              data16 xchg %ax,%ax
  4006e0:   66 66 90              data16 xchg %ax,%ax
  4006e3:   66 66 90              data16 xchg %ax,%ax
  4006e6:   66 66 90              data16 xchg %ax,%ax
  4006e9:   66 66 90              data16 xchg %ax,%ax
  4006ec:   66 66 90              data16 xchg %ax,%ax
  4006ef:   66 66 90              data16 xchg %ax,%ax
  4006f2:   66 66 90              data16 xchg %ax,%ax
  4006f5:   66 66 90              data16 xchg %ax,%ax
  4006f8:   66 66 90              data16 xchg %ax,%ax
  4006fb:   66 66 90              data16 xchg %ax,%ax
  4006fe:   66 66 90              data16 xchg %ax,%ax
  400701:   66 66 90              data16 xchg %ax,%ax
  400704:   66 66 90              data16 xchg %ax,%ax
  400707:   66 66 90              data16 xchg %ax,%ax
  40070a:   66 66 90              data16 xchg %ax,%ax
  40070d:   c3                    retq   
  40070e:   66 90                 xchg   %ax,%ax
\end{verbatim}

There are similarities to the previous method. We see the code starts 
the same way, only the counter and starting address for the stack 
zeroing look suspiciously human-produced. Our suspicion is correct, 
they will be replaced. Below stack cleanup we see exactly 20 
repetitions of a nop-equivalent instruction\footnote{It is supposed to 
exchange the value between the \code{ax} register and itself, therefore 
it has no effect.}. These will also be replaced, and we will see how right 
below:

\begin{verbatim}
  4006b0:   31 c0                 xor    %eax,%eax
  4006b2:   5d                    pop    %rbp
  4006b3:   50                    push   %rax
  4006b4:   48 31 c0              xor    %rax,%rax
  4006b7:   b9 16 02 00 00        mov    $0x216,%ecx
  4006bc:   48 8d bc 24 50 ef ff  lea    -0x10b0(%rsp),%rdi
  4006c3:   ff 
  4006c4:   f3 48 ab              rep stos %rax,%es:(%rdi)
  4006c7:   58                    pop    %rax
  4006c8:   48 c7 44 24 f8 fc ff  movq   $0xfffffffffffffffc,-0x8(%rsp)
  4006cf:   ff ff 
  4006d1:   31 d2                 xor    %edx,%edx
  4006d3:   90                    nop
  4006d4:   48 31 c9              xor    %rcx,%rcx
  4006d7:   89 c0                 mov    %eax,%eax
  4006d9:   90                    nop
  4006da:   66 66 90              data16 xchg %ax,%ax
  4006dd:   66 66 90              data16 xchg %ax,%ax
  4006e0:   66 66 90              data16 xchg %ax,%ax
  4006e3:   66 66 90              data16 xchg %ax,%ax
  4006e6:   66 66 90              data16 xchg %ax,%ax
  4006e9:   66 66 90              data16 xchg %ax,%ax
  4006ec:   66 66 90              data16 xchg %ax,%ax
  4006ef:   66 66 90              data16 xchg %ax,%ax
  4006f2:   66 66 90              data16 xchg %ax,%ax
  4006f5:   66 66 90              data16 xchg %ax,%ax
  4006f8:   66 66 90              data16 xchg %ax,%ax
  4006fb:   66 66 90              data16 xchg %ax,%ax
  4006fe:   66 66 90              data16 xchg %ax,%ax
  400701:   66 66 90              data16 xchg %ax,%ax
  400704:   66 66 90              data16 xchg %ax,%ax
  400707:   66 66 90              data16 xchg %ax,%ax
  40070a:   66 66 90              data16 xchg %ax,%ax
  40070d:   c3                    retq   
  40070e:   66 90                 xchg   %ax,%ax
\end{verbatim}

First thing we notice is the amount of overwritten stack is much higher 
at 4272 bytes. That value is pretty large and makes this method more 
inefficient than the function-based one we discussed before, in 
contrast to claims in the paper\cite{whatyouc}. This value is 
overestimated and composes of three components: a signal handler (4096 
bytes) offset, a red zone offset (128 bytes), and some pre-calculated 
stack usage (48 bytes). Perhaps a there could be some savings in future 
improvements of zerostack.

We also take note that a subset of the 20 \code{xchg} instructions were 
replaced. The substituting code is exactly the same code as in the 
function-based method.

\paragraph{Demonstration of execution}

To complete the experiment, we list a trace showing the cleanup. The 
reference code for XXTEA will serve as the information leaking example 
and \code{gdb} as the examination tool. We execute the program and stop 
at the last line, \code{return 0;}. We set the debugger to repeatedly 
display the contents of the memory where the \code{y} and \code{z} 
local variables reside\footnote{Displaying \code{y} and \code{z} by 
variable name does not work as they go out of scope at the end of the 
function.}.

\begin{verbatim}
(gdb) p &y
$3 = (unsigned int *) 0x7fffffffde40
(gdb) p &z
$4 = (unsigned int *) 0x7fffffffde44
(gdb) display *((unsigned int *) 0x7fffffffde40)
3: *((unsigned int *) 0x7fffffffde40) = 792968749
(gdb) display *((unsigned int *) 0x7fffffffde44)
4: *((unsigned int *) 0x7fffffffde44) = 2860056034
\end{verbatim}

We keep stepping instructions until the \code{rep stos} instruction and 
observe the output:

\begin{verbatim}
B+ |0x400901 <xxtea_enc+369>  pop    %rbp                          
   |0x400902 <xxtea_enc+370>  push   %rax                          
   |0x400903 <xxtea_enc+371>  xor    %rax,%rax                     
   |0x400906 <xxtea_enc+374>  mov    $0x49,%ecx                    
   |0x40090b <xxtea_enc+379>  lea    -0x248(%rsp),%rdi             
  >|0x400913 <xxtea_enc+387>  rep stos %rax,%es:(%rdi)             
   |0x400916 <xxtea_enc+390>  pop    %rax                          
   |0x400917 <xxtea_enc+391>  movq   $0xfffffffffffffffc,-0x8(%rsp)

4: *((unsigned int *) 0x7fffffffde44) = 792968749
3: *((unsigned int *) 0x7fffffffde40) = 2860056034
\end{verbatim}

At some point the output changes the value of the variables to 0. We 
keep stepping until the \code{ret} instruction to confirm:

\begin{verbatim}
   |0x400917 <xxtea_enc+391>  movq   $0xfffffffffffffffc,-0x8(%rsp)
   |0x400920 <xxtea_enc+400>  xor    %rdx,%rdx                     
   |0x400923 <xxtea_enc+403>  xor    %rsi,%rsi                     
   |0x400926 <xxtea_enc+406>  xor    %rcx,%rcx                     
  >|0x400929 <xxtea_enc+409>  retq 

4: *((unsigned int *) 0x7fffffffde44) = 0
3: *((unsigned int *) 0x7fffffffde40) = 0
\end{verbatim}

\paragraph{Another alternative}

A recent standard extension, \term{C11}, has added a new function 
\code{memset\_s} which, unlike its original variant \code{memset}, is 
guaranteed \textbf{not} to be removed by an optimizing compiler. In 
theory, we could use it as a substitute for zerostack, except 
for two shortcomings:

\begin{enumerate}
\item it does not and cannot erase registers, and
\item the programmer would have to make sure it overwrites the right memory range, \textbf{manually}.
\end{enumerate}

The first point may not be too much of an issue since there is a 
general shortage of general-purpose registers on x86 machines and thus 
they get re-used quickly. However, erasing stack can turn difficult: 
the programmer would have to know the location of the stack and its 
correct size. If the sensitive function is being updated during 
development and/or maintenance, the programmer would need to make sure 
the erasure is still correct. On the other hand, the \code{memset\_s} 
function can be easily used to zero out data on the heap, and secondly 
it is supported by a standard and is much more likely to be supported 
by any compiler.

\paragraph{Conclusion of the experiment}

We have shown zerostack is effective and that it erases local 
data as declared and predicted. Therefore we suggest using it or some 
analogous tool for erasing sensitive data from stack and registers. We 
also suggest using \code{memset\_s} in complement. We can recommend 
using zerostack as a substitution for a proper reversible 
platform if the goal is avoiding information leakage.


\chapter{Conclusion, reflection and future work}

%Evaluation of our solution in a broader context, usability in practice 
%and viability of real-world implementation

In this thesis we discussed the advantages of using reversible 
programming language as a way of dealing with the difficulty in 
implementing cryptographic algorithms correctly in relation to 
side-channel vulnerabilities. Although other work already exists (Vale) 
and offers additional guarantee (timing-based leakage protection), it 
is much more difficult to use in practice. Our main contribution is 
therefore exploration of another approach in making sure our 
cryptographic code does not leak any information unintentionally. 
Furthermore, since fully reversible platforms (hardware and full-stack 
software) do not yet exist, we have shown a way of reaching the goal 
with current systems (zerostack).

We have also improved Jana, the Janus interpreter, and suggested further 
development of the language. We have also shown some techniques 
programmers can use when developing reversible code.

Along the way we have developed several symmetric encryption 
algorithms. These are available on the project's GitHub page and are 
released freely under the MIT license.

\paragraph{Usability in practice}

Our code can be used in practice, but has not been thoroughly tested. 
The user would use Jana to translate Janus to C++, as we did in our 
experiment, and use the code as a library, linking it to the rest of 
their project. However, there are some pieces of the puzzle still 
missing: the programmer would still need to supply some (pseudo-) 
random number generator and/or nonce generator. The former can also be 
implemented reversibly (see \term{linear-feedback shift registers}), 
nonce generation is specific to the scenario it is used in.

\paragraph{Future work}

Our and related work can be extended and improved upon in several 
directions:

\begin{itemize}

\item Implementing more algorithms and noting and assessing the 
experience further.

\item Comparing performance between reversible and irreversible code.

\item Stabilizing zerostack and adding the same or similar 
functionality to other compilers.

\item Stabilizing secretgrind to be fully independent on how the 
tracked binary was compiled or what it was linked with.

\item Fixing Vale. The Vale language has a great potential, but its 
actual usability is diminished by its complexity in all its relevant 
parts: from troublesome installation, through language difficulty with 
no proper documentation, through complex and also troublesome 
dependencies and infrastructure, to undocumented, dense and as much as 
unreadable source code.

\item Exploring the same concepts in hardware. There are some 
applications (\eg RFID chips), where lack of information leakage 
through other side-channels is just as important, especially given the 
fact that the attacker has physical access to these devices.

\end{itemize}

% I don't like this chapter, there's no point in repeating those references.
%\chapter{Related work}

%\begin{itemize}
%\item F*/HACL work\cite{ProtzenkoEtal:2017}. 
%\item Vale project\cite{BondEtal:2017:Vale}.
%\item Zerostack and secretgrind\cite{whatyouc}.
%\end{itemize}

\section{Contributions}

These are the contributions from work on this thesis:

\begin{itemize}

\item We have shown reversible computing has other interesting and 
valuable uses other than power consumption savings and quantum 
computing applications; these are better code security and lower 
development costs for bidirectional algorithms.

\item We have suggested new enhancements and improvements to Janus or 
its interpreter Jana. Some of these were implemented alongside the 
research. We have also found and helped fixing several bugs in Jana. 16 
is currently the number of reported issues with Jana, more issues were 
discussed in person.

\item We have published our findings in our paper\cite{revcry-paper}, which 
has been accepted to the Reversible Computation Workshop 2018. We have 
also presented our research at the \term{resund Security Day Spring 
2018} conference.

\item All the work is freely accessible on 
GitHub\cite{revcry-github}. This includes the algorithms, 
supporting files like other source codes, binaries and listings, this 
thesis source files and the wiki resources.

\item \todo{what else?}

\end{itemize}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% BIBLIOGRAPHY
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\cleardoublepage
\renewcommand{\sc}[1]{\textsc{#1}}
\nocite{*}
\bibliographystyle{acm}
\bibliography{bibliography}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Appendix
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\cleardoublepage
\appendix

% Updates section names to remove . (e.g. a section is named A1 instead of A.1)
\renewcommand{\thesection}{\thechapter\arabic{section}}

\chapter{The extra}


% To make sure that the backside print of the dissertation is a white page
\newpage ~
\thispagestyle{empty}



\end{document}
